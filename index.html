<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title>Jianhao Ma</title>
</head>
<body>
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
<div class="menu-category">Home</div>
<div class="menu-item"><a href="index.html" class="current">About&nbsp;me</a></div>
<div class="menu-category">Research</div>
<div class="menu-item"><a href="publications.html">Publications</a></div>
</td>
<td id="layout-content">
<div id="toptitle">
<h1>Jianhao Ma</h1>
</div>
<table class="imgtable"><tr><td>
<img src="photos/jianhao.jpg" alt="alt text" width="326px" height="245px" />&nbsp;</td>
<td align="left"><p><a href="https://tennysonabc.github.io" target=&ldquo;blank&rdquo;>Jianhao Ma</a> <br /> 
Ph.D. candidate at <a href="https://ioe.engin.umich.edu" target=&ldquo;blank&rdquo;>Department of Industrial and Operational Engineering</a> <br />
University of Michigan, Ann Arbor, United States <br />
<br />
Email: jianhao [at] umich [dot] edu <br />
[<a href="https://scholar.google.com/citations?user=7ubAY0UAAAAJ&amp;hl=en" target=&ldquo;blank&rdquo;>Google Scholar</a>][<a href="CV_jianhao.pdf" target=&ldquo;blank&rdquo;>Curriculem Vitae</a>] <br />
</p>
</td></tr></table>
<h2>About me</h2>
<p>I'm currently a third-year Ph.D. candidate at Umich. I am fortunate to be advised by Professor <a href="https://fattahi.engin.umich.edu" target=&ldquo;blank&rdquo;>Salar Fattahi</a>. My research focus on understanding the pratical success of gradient-based algorithms based on trajectory analysis. I welcome future collaborations. Please feel free to contact me via Email!
</p>
<h2>Research Interests </h2>
<ul>
<li><p>General optimization and generalization in modern machine learning.
</p>
</li>
<li><p>Matrix factorization and tensor decomposition.
</p>
</li>
<li><p>Empirical process and its application.
</p>
</li>
</ul>
<h2>Preprints</h2>
<ul>
<li><p>Behind the Scenes of Gradient Descent: A Trajectory Analysis via Basis Function Decomposition [<a href="https://arxiv.org/abs/2210.00346" target=&ldquo;blank&rdquo;>arxiv</a>] <br />
<b>Jianhao Ma</b>, Lingjun Guo, Salar Fattahi <br /> 
</p>
</li>
<li><p>Global Convergence of Sub-gradient Method for Robust Matrix Recovery: Small Initialization, Noisy Measurements, and Over-parameterization [<a href="https://arxiv.org/abs/2202.08788" target=&ldquo;blank&rdquo;>arxiv</a>] <br />
Conditionally accepted at Journal of Machine Learning Research (JMLR) <br />
<b>Jianhao Ma</b>, Salar Fattahi <br /> 
</p>
</li>
</ul>
<h2>Publications</h2>
<ul>
<li><p>Blessing of Nonconvexity in Deep Linear Models: Depth Flattens the Optimization Landscape Around the True Solution [<a href="https://arxiv.org/abs/2207.07612" target=&ldquo;blank&rdquo;>arxiv</a>] <br />
Advances in Neural Information Processing Systems (NeurIPS) 2022 (<font color="red">Spotlight</font>) <br />
<b>Jianhao Ma</b>, Salar Fattahi <br /> 
</p>
</li>
<li><p>Towards Understanding Generalization via Decomposing Excess Risk Dynamics [<a href="https://openreview.net/forum?id=rS9-7AuPKWK" target=&ldquo;blank&rdquo;>paper</a>] [<a href="https://arxiv.org/abs/2106.06153" target=&ldquo;blank&rdquo;>arxiv</a>] <br /> 
Jiaye Teng*, <b>Jianhao Ma</b>*, Yang Yuan <br /> 
International Conference on Learning Representations (ICLR) 2022 <br /> 
</p>
</li>
<li><p>Sign-RIP: A Robust Restricted Isometry Property for Low-rank Matrix Recovery [<a href="https://opt-ml.org/papers/2021/paper14.pdf" target=&ldquo;blank&rdquo;>paper</a>][<a href="https://arxiv.org/abs/2102.02969" target=&ldquo;blank&rdquo;>arxiv</a>] <br /> 
<b>Jianhao Ma</b>, Salar Fattahi <br /> 
NeurIPS Workshop on Optimization for Machine Learning, 2021 <br />
</p>
</li>
</ul>
<h2>News</h2>
<ul>
<li><p><b>October 2022:</b> Receive the NeurIPS 2022 Scholar Award.
</p>
</li>
<li><p><b>October 2022:</b> New paper on the convergence of gradient descent: <a href="https://arxiv.org/abs/2210.00346" target=&ldquo;blank&rdquo;>Behind the Scenes of Gradient Descent: A Trajectory Analysis via Basis Function Decomposition</a>. Joint work with Lingjun Guo and Salar Fattahi.
</p>
</li>
<li><p><b>September 2022:</b> Our paper <a href="https://arxiv.org/abs/2207.07612" target=&ldquo;blank&rdquo;>Blessing of Nonconvexity in Deep Linear Models: Depth Flattens the Optimization Landscape Around the True Solution</a> has been accepted in NeurIPS 2022! Joint work with my advisor Salar Fattahi.
</p>
</li>
<li><p><b>June 2022:</b> I will give a talk in INFORMS Annual Meeting, 2022.
</p>
</li>
</ul>
<h2>Conference Services</h2>
<p>Reviewer of ICML, NeurIPS, ICLR. 
</p>
</td>
</tr>
</table>
</body>
</html>
